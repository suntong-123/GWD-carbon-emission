import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from scipy.spatial.distance import jensenshannon
#è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆWindowsç³»ç»Ÿå¸¸ç”¨å­—ä½“ç¤ºä¾‹ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']  # é»‘ä½“
# è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['axes.unicode_minus'] = False
# è¯»å–æ•°æ®
df = pd.read_csv(r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\diabetes_imputed7 -ï¼ˆ5-1000ï¼‰ï¼ˆä¼˜åŒ–ç‰¹å¾11ï¼‰.csv")

# å¤„ç†åˆ—åï¼Œå»é™¤å¯èƒ½çš„ç©ºæ ¼
df.columns = df.columns.str.strip()

# é€‰æ‹©æ•°å€¼åˆ—
numeric_cols = df.select_dtypes(include=['number']).columns
df_numeric = df[numeric_cols]

# ç›®æ ‡åˆ—
target_col = "SkinThickness"
assert target_col in df_numeric.columns, f"åˆ— {target_col} ä¸åœ¨æ•°æ®é›†ä¸­ï¼Œå®é™…åˆ—åï¼š{df_numeric.columns.tolist()}"

# å®šä¹‰ RMSE è®¡ç®—å‡½æ•°
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))
def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# ========== ğŸ”¥ ç‰¹å¾å½’ä¸€åŒ– ==========
scaler = StandardScaler()
X = df_numeric.drop(target_col, axis=1)  # ç‰¹å¾
y = df_numeric[target_col]  # ç›®æ ‡å˜é‡

X_scaled = scaler.fit_transform(X)  # å½’ä¸€åŒ–ç‰¹å¾

# ========== ğŸ”¥ åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† ==========
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# ========== ğŸ”¥ äº¤å‰éªŒè¯è®­ç»ƒ MLP ç¥ç»ç½‘ç»œ ==========
kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5æŠ˜äº¤å‰éªŒè¯
mlp_model = MLPRegressor(hidden_layer_sizes=(128, 64), activation="relu", solver="adam",
                         learning_rate_init=0.01, max_iter=500, random_state=42)

rmse_scores = -cross_val_score(mlp_model, X_train, y_train, cv=kf, scoring="neg_root_mean_squared_error")
mean_rmse = np.mean(rmse_scores)  # äº¤å‰éªŒè¯ RMSE

# è®­ç»ƒæœ€ç»ˆæ¨¡å‹
mlp_model.fit(X_train, y_train)

# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_pred = mlp_model.predict(X_test)

# è®¡ç®—æœ€ç»ˆæµ‹è¯•é›† RMSE å’Œ RÂ²
final_rmse = rmse(y_test, y_pred)
final_mae = mae(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
pearson_corr = np.corrcoef(y_test, y_pred)[0, 1]

y_test_hist, _ = np.histogram(y_test, bins=20, density=True)
y_pred_hist, _ = np.histogram(y_pred, bins=20, density=True)
jsd = jensenshannon(y_test_hist, y_pred_hist)

print(f"5æŠ˜äº¤å‰éªŒè¯ RMSE: {mean_rmse:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ RMSE: {final_rmse:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ RÂ²: {r2:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ MAE: {final_mae:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ Pearson ç›¸å…³ç³»æ•°: {pearson_corr:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ JSD: {jsd:.4f}")

# ========== ğŸ“ˆ ç»˜åˆ¶çœŸå®å€¼ vs é¢„æµ‹å€¼çš„å›å½’å›¾ï¼Œå¹¶æ ‡æ³¨ RÂ² ==========
plt.figure(figsize=(8, 6))
sns.regplot(x=y_test, y=y_pred, scatter_kws={"color": "blue", "alpha": 0.5}, line_kws={"color": "red"})

# æ ‡æ³¨ RÂ² å€¼
plt.text(x=min(y_test), y=max(y_pred), s=f"$R^2$ = {r2:.4f}", fontsize=12, color="black",
         bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))

plt.xlabel("True Value")
plt.ylabel("Predicted Value")
plt.title(f"MLP Regression (Normalized): True vs Predicted")
plt.grid(True)

plt.tight_layout()
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\MLP\MLP Regression (Normalized) True vs Predicted.png",
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 1==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('MLP Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\MLP\MLP_kde_plot_1.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 2==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(8, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)
cbar = plt.colorbar(sc)
cbar.set_label('Density')

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('MLP Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\MLP\MLP_kde_plot_2.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦å›¾ ==========
fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sns.kdeplot(
    x=y_test,
    y=y_pred,
    cmap='coolwarm',
    shade=True,
    ax=ax
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('MLP Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\MLP\MLP_kde_plot_3.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()