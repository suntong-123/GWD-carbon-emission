import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import gaussian_kde
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from scipy.spatial.distance import jensenshannon
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
#å»ºè®®å¯¹>10,000æ ·æœ¬æ•°æ®é‡‡ç”¨çº¿æ€§æ ¸
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç† ---------------------------------
file_path = r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\diabetes_imputed7 -ï¼ˆ5-1000ï¼‰ï¼ˆä¼˜åŒ–ç‰¹å¾11ï¼‰.csv"

selected_cols = pd.read_csv(file_path, nrows=0).columns[5:]
df = pd.read_csv(file_path, usecols=selected_cols)

df.columns = df.columns.str.strip()
numeric_cols = df.select_dtypes(include=['number']).columns
df_numeric = df[numeric_cols]

target_col = "SkinThickness"
assert target_col in df_numeric.columns, f"åˆ— {target_col} ä¸å­˜åœ¨ï¼Œå®é™…åˆ—åï¼š{df_numeric.columns.tolist()}"

# æ•°æ®åˆ†å‰² --------------------------------------
X = df_numeric.drop(target_col, axis=1)
y = df_numeric[target_col]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SVMæ¨¡å‹è®¾ç½®ï¼ˆåŒ…å«æ ‡å‡†åŒ–ï¼‰--------------------------
kf = KFold(n_splits=5, shuffle=True, random_state=42)
svm_pipeline = make_pipeline(
    StandardScaler(),
    SVR(kernel='rbf')
)

# å‚æ•°ç½‘æ ¼è®¾ç½®ï¼ˆæ‰©å±•äº†æ›´å¤šå‚æ•°ï¼‰
params = {
    'svr__C': [0.1, 1, 10, 100],  # å¢åŠ æ›´å¤šCå‚æ•°
    'svr__epsilon': [0.01, 0.1, 0.2, 0.3],  # å¢åŠ epsiloné€‰é¡¹
    'svr__gamma': ['scale', 'auto', 0.01, 0.1]  # æ–°å¢gammaå‚æ•°æ¢ç´¢
}

grid = GridSearchCV(
    estimator=svm_pipeline,
    param_grid=params,
    cv=kf,  # ä½¿ç”¨é¢„å®šä¹‰çš„äº¤å‰éªŒè¯æ–¹æ³•
    scoring='neg_root_mean_squared_error',  # è®¾ç½®è¯„åˆ†æŒ‡æ ‡ä¸ºRMSE
    verbose=1  # æ˜¾ç¤ºæœç´¢è¿‡ç¨‹
)

# æ‰§è¡Œç½‘æ ¼æœç´¢
grid.fit(X_train, y_train)

# è·å–æœ€ä½³æ¨¡å‹
best_model = grid.best_estimator_
print(f"æœ€å¥½å‚æ•°ç»„åˆï¼š{grid.best_params_}")

# éªŒè¯ç»“æœè¯„ä¼°
y_pred = best_model.predict(X_test)
final_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
final_mae = np.mean(np.abs(y_test - y_pred))
r2 = r2_score(y_test, y_pred)
pearson_corr = np.corrcoef(y_test, y_pred)[0, 1]

y_test_hist, _ = np.histogram(y_test, bins=20, density=True)
y_pred_hist, _ = np.histogram(y_pred, bins=20, density=True)
jsd = jensenshannon(y_test_hist, y_pred_hist)

# è¾“å‡ºè¯¦ç»†è¯„ä»·æŒ‡æ ‡
print("\nä¼˜åŒ–åæ¨¡å‹æ€§èƒ½ï¼š")
print(f"æµ‹è¯•é›†RMSE: {final_rmse:.4f}")
print(f"æµ‹è¯•é›†RÂ²: {r2:.4f}")
print(f"æµ‹è¯•é›†MAE: {final_mae:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ Pearson ç›¸å…³ç³»æ•°: {pearson_corr:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ JSD: {jsd:.4f}")

# å›å½’å¯è§†åŒ– ------------------------------------
plt.figure(figsize=(8, 6))
sns.regplot(x=y_test, y=y_pred,
           scatter_kws={"color": "blue", "alpha": 0.5},
           line_kws={"color": "red"})
plt.text(x=min(y_test), y=max(y_pred),
        s=f"$R^2$ = {r2:.4f}",
        fontsize=12,
        bbox=dict(facecolor='white', edgecolor='black'))
plt.xlabel("True Value")
plt.ylabel("Predicted Value")
plt.title("SVM Regression: True vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\SVM\SVM Regression True vs Predicted.png",
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 1==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('SVM Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\SVM\SVM_kde_plot_1.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 2==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(8, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)
cbar = plt.colorbar(sc)
cbar.set_label('Density')

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('SVM Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\SVM\SVM_kde_plot_2.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦å›¾ ==========
fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sns.kdeplot(
    x=y_test,
    y=y_pred,
    cmap='coolwarm',
    shade=True,
    ax=ax
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('SVM Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\SVM\SVM_kde_plot_3.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()