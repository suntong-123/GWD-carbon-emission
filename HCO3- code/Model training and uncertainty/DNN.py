import pandas as pd
import numpy as np
from scipy.stats import gaussian_kde
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from keras import layers, models, callbacks
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from scipy.spatial.distance import jensenshannon
import warnings

 #è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆWindowsç³»ç»Ÿå¸¸ç”¨å­—ä½“ç¤ºä¾‹ï¼‰
plt.rcParams['font.sans-serif'] = ['SimHei']  # é»‘ä½“
# è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
plt.rcParams['axes.unicode_minus'] = False
# è¯»å–æ•°æ®
df = pd.read_csv(r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\diabetes_imputed7 -ï¼ˆ5-1000ï¼‰ï¼ˆä¼˜åŒ–ç‰¹å¾11ï¼‰.csv")

# å¤„ç†åˆ—åï¼Œå»é™¤å¯èƒ½çš„ç©ºæ ¼
df.columns = df.columns.str.strip()

# é€‰æ‹©æ•°å€¼åˆ—
numeric_cols = df.select_dtypes(include=['number']).columns
df_numeric = df[numeric_cols]

# ç›®æ ‡åˆ—
target_col = "SkinThickness"
assert target_col in df_numeric.columns, f"åˆ— {target_col} ä¸åœ¨æ•°æ®é›†ä¸­ï¼Œå®é™…åˆ—åï¼š{df_numeric.columns.tolist()}"

print(f"""
=============================================
é¢„æµ‹å˜é‡ ({target_col}) æ–¹å·®åˆ†æ:
+ æ€»ä½“æ•°æ®é‡: {len(df_numeric)}
+ æ€»ä½“å‡å€¼: {df_numeric[target_col].mean():.4f}
+ æ€»ä½“æ–¹å·® (ÏƒÂ², ddof=0): {np.var(df_numeric[target_col]):.4f}
=============================================
""")

# å®šä¹‰ RMSE è®¡ç®—å‡½æ•°
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# ========== ğŸš€ æ·±åº¦å­¦ä¹ ä¿®æ”¹éƒ¨åˆ† ==========
# ç‰¹å¾å½’ä¸€åŒ–
scaler = StandardScaler()
X = df_numeric.drop(target_col, axis=1)
y = df_numeric[target_col].values

X_scaled = scaler.fit_transform(X)

# æ•°æ®åˆ’åˆ†
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)


# æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹
def build_model(input_shape):
    model = models.Sequential([
        layers.Dense(256, activation='relu', input_shape=(input_shape,)),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),
        layers.Dense(1)  # å›å½’ä»»åŠ¡æ— æ¿€æ´»å‡½æ•°
    ])

    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(
        optimizer=optimizer,
        loss='mse',
        metrics=['mae']
    )
    return model


# æ—©åœå’Œä¼˜åŒ–å™¨å›è°ƒ
early_stopping = callbacks.EarlyStopping(
    monitor='val_loss',
    patience=50,
    restore_best_weights=True
)

# è®­ç»ƒé…ç½®
model = build_model(X_train.shape[1])
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=1000,
    batch_size=32,
    verbose=1,
    callbacks=[early_stopping]
)

# ========== ğŸ“Š è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ– ==========
plt.figure(figsize=(12, 4))

# ç»˜åˆ¶æŸå¤±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title('Training and Validation Loss Curve')

# ç»˜åˆ¶MAEæ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.title('Training and Validation MAE Curve')

plt.tight_layout()
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\DNN\training_curves.png",
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ” æ¨¡å‹è¯„ä¼° ==========
y_pred = model.predict(X_test).flatten()

final_rmse = rmse(y_test, y_pred)
final_mae = mae(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
pearson_corr = np.corrcoef(y_test, y_pred)[0, 1]

y_test_hist, _ = np.histogram(y_test, bins=20, density=True)
y_pred_hist, _ = np.histogram(y_pred, bins=20, density=True)
jsd = jensenshannon(y_test_hist, y_pred_hist)

print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ RMSE: {final_rmse:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ RÂ²: {r2:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ MAE: {final_mae:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ Pearson ç›¸å…³ç³»æ•°: {pearson_corr:.4f}")
print(f"æœ€ç»ˆæµ‹è¯•é›†ä¸Šçš„ JSD: {jsd:.4f}")

# ========== ğŸ“ˆ å›å½’å¯è§†åŒ– ==========
plt.figure(figsize=(10, 6))
sns.regplot(x=y_test, y=y_pred,
            scatter_kws={"color": "blue", "alpha": 0.5},
            line_kws={"color": "red"})

plt.text(x=min(y_test), y=max(y_pred),
         s=f"$R^2$ = {r2:.4f}",
         fontsize=12,
         color="black",
         bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.3'))

plt.xlabel("True Value")
plt.ylabel("Predicted Value")
plt.title("DNN Performance: True vs Predicted")
plt.grid(True)
plt.tight_layout()
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\DNN\DNN Performance.png",
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 1==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('DNN Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\DNN\DNN_kde_plot_1.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦-æ•£ç‚¹å›¾ 2==========
xy = np.vstack([y_test, y_pred])
kde = gaussian_kde(xy)
density = kde(xy)

fig, ax = plt.subplots(figsize=(8, 6), dpi=300)

sc = plt.scatter(
    x=y_test,
    y=y_pred,
    c=density,
    cmap='coolwarm',
    alpha=0.5,
    edgecolor='none',
)
cbar = plt.colorbar(sc)
cbar.set_label('Density')

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('DNN Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\DNN\DNN_kde_plot_2.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()

# ========== ğŸ“ˆ æ ¸å¯†åº¦å›¾ ==========
fig, ax = plt.subplots(figsize=(6, 6), dpi=300)

sns.kdeplot(
    x=y_test,
    y=y_pred,
    cmap='coolwarm',
    shade=True,
    ax=ax
)

# æ·»åŠ å¯¹è§’çº¿å‚è€ƒçº¿
x_min = min(y_test.min(), y_pred.min())
x_max = max(y_test.max(), y_pred.max())
ax.plot([x_min, x_max], [x_min, x_max], 'k--')

# è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
ax.set_xlabel('True Value', fontsize=16, fontweight='bold')
ax.set_ylabel('Predicted Value', fontsize=16, fontweight='bold')
ax.set_title('DNN Model', fontsize=18, fontweight='bold')

ax.grid(True)
ax.legend().set_visible(False)

test_metrics_text = (
    f"$R^2$:{r2:.4f}\n"
    f"RMSE:{final_rmse:.4f}\n"
    f"MAE:{final_mae:.4f}\n"
    f"Pearson's r:{pearson_corr:.4f}\n"
    f"JSD:{jsd:.4f}"
)

# æ·»åŠ RÂ²æŒ‡æ ‡æ–‡æœ¬
ax.text(
    0.05, 0.95,
    f"{test_metrics_text}",
    transform=ax.transAxes,
    fontsize=15,
    fontweight='bold',
    verticalalignment='top',
    horizontalalignment='left',
    color='black',
)

# ä¿å­˜å›¾åƒå¹¶æ˜¾ç¤º
plt.savefig(
    r"C:\Users\doomsday\Desktop\å¹²æ´»\2025.4.12\DNN\DNN_kde_plot_3.png",  # æ ¹æ®è·¯å¾„ä¿®æ”¹
    format='png',
    dpi=300,
    bbox_inches='tight'
)
plt.show()